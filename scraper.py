#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import re
import json
import os
from datetime import datetime
from typing import List, Dict
from bs4 import BeautifulSoup

class LTO539Scraper:
    def __init__(self):
        self.base_url = "https://www.pilio.idv.tw/lto539/list539BIG.asp"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def fetch_page(self, page: int = 1, order_by: str = "new") -> str:
        """æŠ“å–æŒ‡å®šé é¢çš„å…§å®¹"""
        params = {
            'indexpage': page,
            'orderby': order_by
        }
        
        try:
            response = requests.get(self.base_url, params=params, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"Error fetching page {page}: {e}")
            return ""
    
    def parse_lottery_data(self, html_content: str) -> List[Dict]:
        """è§£æé–‹çè³‡æ–™"""
        soup = BeautifulSoup(html_content, 'html.parser')
        text_content = soup.get_text()

        # æ”¯æ´æ–°èˆŠå…©ç¨®æ ¼å¼ï¼š
        # æ–°æ ¼å¼ï¼šé–‹çæ—¥æœŸ:2025/10/15(ä¸‰)
        # èˆŠæ ¼å¼ï¼š2025/09/29
        pattern = r'(?:é–‹çæ—¥æœŸ:)?(\d{4}/\d{2}/\d{2})(?:\([ä¸€äºŒä¸‰å››äº”å…­æ—¥]\))?\s*\r?\n\s*\r?\n\s*(\d{1,2},\s*\d{1,2},\s*\d{1,2},\s*\d{1,2},\s*\d{1,2})'
        matches = re.findall(pattern, text_content, re.DOTALL)
        
        lottery_data = []
        
        for date_str, numbers_str in matches:
            try:
                # æ¸…ç†è™Ÿç¢¼å­—ä¸²ï¼Œç§»é™¤æ‰€æœ‰ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                numbers_str = re.sub(r'[\s\xa0]+', '', numbers_str)  # ç§»é™¤æ‰€æœ‰ç©ºæ ¼å’Œ\xa0
                numbers = [int(num.strip()) for num in numbers_str.split(',') if num.strip()]
                
                if len(numbers) == 5:  # ç¢ºä¿æœ‰5å€‹è™Ÿç¢¼
                    lottery_data.append({
                        'date': date_str,
                        'numbers': numbers,
                        'timestamp': datetime.strptime(date_str, '%Y/%m/%d').isoformat()
                    })
                    
            except Exception as e:
                print(f"Error parsing date {date_str} or numbers {numbers_str}: {e}")
                continue
        
        return lottery_data
    
    def load_existing_data(self, filename: str = "lottery_data.json") -> List[Dict]:
        """è¼‰å…¥ç¾æœ‰çš„è³‡æ–™"""
        if os.path.exists(filename):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get('data', [])
            except Exception as e:
                print(f"Error loading existing data: {e}")
        return []
    
    def scrape_recent_data(self, pages: int = 3) -> List[Dict]:
        """åªæŠ“å–æœ€è¿‘çš„è³‡æ–™"""
        all_data = []
        
        for page in range(1, pages + 1):
            print(f"Scraping page {page}/{pages}...")
            html_content = self.fetch_page(page)
            
            if not html_content:
                print(f"Failed to fetch page {page}")
                continue
            
            page_data = self.parse_lottery_data(html_content)
            all_data.extend(page_data)
            
            # é¿å…éåº¦é »ç¹è«‹æ±‚
            import time
            time.sleep(2)
        
        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        all_data.sort(key=lambda x: x['timestamp'], reverse=True)
        
        return all_data
    
    def merge_and_deduplicate(self, existing_data: List[Dict], new_data: List[Dict]) -> List[Dict]:
        """åˆä½µä¸¦å»é‡è³‡æ–™"""
        # å»ºç«‹æ—¥æœŸç´¢å¼•ä¾†å¿«é€ŸæŸ¥æ‰¾
        existing_dates = {item['date'] for item in existing_data}
        
        # åªä¿ç•™æ–°çš„è³‡æ–™
        truly_new_data = [item for item in new_data if item['date'] not in existing_dates]
        
        # åˆä½µè³‡æ–™
        merged_data = truly_new_data + existing_data
        
        # æŒ‰æ—¥æœŸæ’åº
        merged_data.sort(key=lambda x: x['timestamp'], reverse=True)
        
        return merged_data
    
    def save_to_json(self, data: List[Dict], filename: str = "lottery_data.json"):
        """å°‡è³‡æ–™å„²å­˜ç‚ºJSONæª”æ¡ˆ"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'last_updated': datetime.now().isoformat(),
                    'total_records': len(data),
                    'data': data
                }, f, ensure_ascii=False, indent=2)
            print(f"Data saved to {filename} with {len(data)} records")
        except Exception as e:
            print(f"Error saving data: {e}")

    def send_discord_notification(self, latest_date: str, total_records: int, new_records: int = 0):
        """ç™¼é€ Discord é€šçŸ¥

        Args:
            latest_date: æœ€æ–°è³‡æ–™çš„æ—¥æœŸ
            total_records: ç¸½è³‡æ–™ç­†æ•¸
            new_records: æœ¬æ¬¡æ–°å¢çš„è³‡æ–™ç­†æ•¸
        """
        # å¾ç’°å¢ƒè®Šæ•¸è®€å– webhook URL
        webhook_url = os.getenv('DISCORD_WEBHOOK_URL')

        if not webhook_url:
            print("Warning: DISCORD_WEBHOOK_URL environment variable not set. Skipping notification.")
            return

        # å»ºç«‹é€šçŸ¥è¨Šæ¯
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        message = {
            "content": f"âœ… **ä»Šå½©539è³‡æ–™æ›´æ–°å®Œæˆ**\n\n"
                      f"ğŸ“… æœ€æ–°é–‹çæ—¥æœŸï¼š`{latest_date}`\n"
                      f"ğŸ“Š ç¸½è³‡æ–™ç­†æ•¸ï¼š**{total_records}** ç­†\n"
                      f"ğŸ†• æœ¬æ¬¡æ–°å¢ï¼š**{new_records}** ç­†\n"
                      f"ğŸ• æ›´æ–°æ™‚é–“ï¼š`{current_time}`",
            "username": "ä»Šå½©539è³‡æ–™ç›£æ§"
        }

        try:
            response = requests.post(
                webhook_url,
                json=message,
                headers={'Content-Type': 'application/json'},
                timeout=10
            )
            response.raise_for_status()
            print(f"Discord notification sent successfully (Status: {response.status_code})")

        except requests.exceptions.RequestException as e:
            print(f"Error sending Discord notification: {e}")

def main():
    scraper = LTO539Scraper()

    # è¼‰å…¥ç¾æœ‰è³‡æ–™
    print("Loading existing data...")
    existing_data = scraper.load_existing_data()
    print(f"Found {len(existing_data)} existing records")

    # æŠ“å–æœ€è¿‘çš„è³‡æ–™
    print("Scraping recent data...")
    new_data = scraper.scrape_recent_data(pages=3)
    print(f"Scraped {len(new_data)} records from recent pages")

    # åˆä½µä¸¦å»é‡
    print("Merging and deduplicating...")
    merged_data = scraper.merge_and_deduplicate(existing_data, new_data)

    # è¨ˆç®—æ–°å¢çš„è³‡æ–™ç­†æ•¸
    new_records_count = len(merged_data) - len(existing_data)

    # å„²å­˜æ›´æ–°å¾Œçš„è³‡æ–™
    scraper.save_to_json(merged_data)

    print(f"Update complete. Total records: {len(merged_data)}")

    # å–å¾—æœ€æ–°è³‡æ–™æ—¥æœŸ
    if merged_data:
        latest_date = merged_data[0]['date']

        # ç™¼é€ Discord é€šçŸ¥
        print("\nSending Discord notification...")
        scraper.send_discord_notification(
            latest_date=latest_date,
            total_records=len(merged_data),
            new_records=new_records_count
        )
    else:
        print("âš ï¸  No data available to send notification")

if __name__ == "__main__":
    main()